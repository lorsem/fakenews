{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pymongo as pym\n",
    "import seaborn as sns\n",
    "from bson.objectid import ObjectId\n",
    "from bson import Int64\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from pylab import *\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import independent_cascade as ind_casc\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "from matplotlib.cbook import flatten\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "from matplotlib.cbook import flatten\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import statistics\n",
    "\n",
    "from sklearn import model_selection\n",
    "import sklearn\n",
    "from sklearn.cluster import DBSCAN, KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to localhost instance (default)\n",
    "client = MongoClient()\n",
    "db = client.faken\n",
    "coll = db.bias_tweets_unique\n",
    "cursor = db.bias_tweets_unique.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find tweets that have #FakeNews related keywords\n",
    "cursor_fakenews = coll.find( {\"$or\" :  [{'text': {\"$regex\": '#fakenews', \"$options\": 'i' }},\n",
    "                                {'text': {\"$regex\": 'fakenews', \"$options\": 'i' }},\n",
    "                                {'text': {\"$regex\": 'fake-news', \"$options\": 'i' }},\n",
    "                                {'text': {\"$regex\": '#fake-news', \"$options\": 'i' }},\n",
    "                                {'text': {\"$regex\": 'posttruth', \"$options\": 'i' }},\n",
    "                                {'text': {\"$regex\": '#post-truth', \"$options\": 'i' }},\n",
    "                                {'text': {\"$regex\": 'alternativefact', \"$options\": 'i' }},\n",
    "                                {'text': {\"$regex\": '#alternativefact', \"$options\": 'i' }},\n",
    "                                {'text': {\"$regex\": 'alternative-fact', \"$options\": 'i' }},\n",
    "                                {'text': {\"$regex\": '#alternative-fact', \"$options\": 'i' }} ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same query as before as we need it for both DBs later: UIDs are different!\n",
    "cursor_fakenews_innon = db.nonfakes.find( {\"$or\" :  [{'text': {\"$regex\": '#fakenews', \"$options\": 'i' }},\n",
    "                                {'text': {\"$regex\": 'fakenews', \"$options\": 'i' }},\n",
    "                                {'text': {\"$regex\": 'fake-news', \"$options\": 'i' }},\n",
    "                                {'text': {\"$regex\": '#fake-news', \"$options\": 'i' }},\n",
    "                                {'text': {\"$regex\": 'posttruth', \"$options\": 'i' }},\n",
    "                                {'text': {\"$regex\": '#post-truth', \"$options\": 'i' }},\n",
    "                                {'text': {\"$regex\": 'alternativefact', \"$options\": 'i' }},\n",
    "                                {'text': {\"$regex\": '#alternativefact', \"$options\": 'i' }},\n",
    "                                {'text': {\"$regex\": 'alternative-fact', \"$options\": 'i' }},\n",
    "                                {'text': {\"$regex\": '#alternative-fact', \"$options\": 'i' }} ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd90() # Not to rerun\n",
    "for eleme in cursor_fakenews_innon:\n",
    "    db.nonfakes.delete_one({'_id': ObjectId(str(eleme['_id']))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdafas() # Not rerun\n",
    "for element in cursor_fakenews.rewind():\n",
    "    db.fakes.insert_one(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdafasjdw() # Not to rerun\n",
    "    print(db.nonfakes.count())\n",
    "    for element in cursor_fakenews_innon.rewind():\n",
    "        if db.nonfakes.delete_many({'_id': ObjectId(str(element['_id']))}).deleted_count != 0:\n",
    "            print(1)\n",
    "\n",
    "    print(db.nonfakes.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retweet statistics\n",
    "def times_timelist(theCursor):\n",
    "    times = dict()\n",
    "    timelist = list()\n",
    "    for el in theCursor:\n",
    "        try:\n",
    "            timelist.append(el['retweet_reaction_time_sec'])\n",
    "        except KeyError:\n",
    "            continue\n",
    "        try:\n",
    "            times[el['retweet_reaction_time_sec']] += 1\n",
    "        except KeyError:\n",
    "            times[el['retweet_reaction_time_sec']] = 1\n",
    "    return (times, timelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_timelist_computed_nonfakes = times_timelist(db.nonfakes.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_timelist_computed_fakes = times_timelist(db.fakes.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(times_timelist_computed_nonfakes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn graphs\n",
    "sns.set()\n",
    "ax = sns.distplot(list(filter(lambda x: x<40000, times_timelist_computed_fakes[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "ax = sns.distplot(list(filter(lambda x: x<1000000, times_timelist_computed_nonfakes[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit to retweet delay data\n",
    "x_fake = list()\n",
    "y_fake = list()\n",
    "for k in sorted(times_timelist_computed_fakes[0].keys()):\n",
    "    if k<4000:\n",
    "        x_fake.append(k)\n",
    "        y_fake.append(times_timelist_computed_fakes[0][k])\n",
    "\n",
    "x_fake = np.array(x_fake)\n",
    "y_fake = np.array(y_fake)\n",
    "\n",
    "def func(x, a, c, d):\n",
    "    return a*np.exp(-c*x)+d\n",
    "\n",
    "popt_f, pcov_f = curve_fit(func, x_fake, y_fake, p0=(45, 1e-12, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nfake = list()\n",
    "y_nfake = list()\n",
    "for k in sorted(times_timelist_computed_nonfakes[0].keys()):\n",
    "    if k<1000000:\n",
    "        x_nfake.append(k)\n",
    "        y_nfake.append(times_timelist_computed_nonfakes[0][k])\n",
    "\n",
    "x_nfake = np.array(x_nfake)\n",
    "y_nfake = np.array(y_nfake)\n",
    "\n",
    "def func(x, a, c, d):\n",
    "    return a*np.exp(-c*x)+d\n",
    "\n",
    "popt_nf, pcov_nf = curve_fit(func, x_nfake, y_nfake, p0=(45, 1e-12, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(popt_nf)\n",
    "print(popt_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(xx_nf, np.array(list(map(lambda x: x/max(yy_nf),yy_nf))))# max > 8000\n",
    "plot(xx_f,  np.array(list(map(lambda x: x/max(yy_f),yy_f)))) #max in 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create retweet graphs and helper functions\n",
    "def make_rt_graph_for_fakesres(cursor, max_seconds=False):\n",
    "    graph = nx.DiGraph()\n",
    "    graph_dict = dict()\n",
    "    for smth in cursor:\n",
    "        orig_id = ''\n",
    "        graph_dict[str(smth['msg_id'])] = smth\n",
    "        try: \n",
    "            orig_id = str(smth['retweeted_msg_id'])\n",
    "            orig_id = orig_id.split('_')[-1]\n",
    "            if max_seconds and smth['retweet_reaction_time_sec'] > max_seconds: # 5 minutes\n",
    "                continue\n",
    "        except KeyError: \n",
    "            continue\n",
    "        graph.add_edges_from([(orig_id, str(smth['msg_id']))])\n",
    "        \n",
    "        el = smth\n",
    "        try:\n",
    "            supp=list(flatten(el[\"topics\"]))\n",
    "        except KeyError:\n",
    "            continue\n",
    "        if \"TRUMP\" in supp:\n",
    "            el[\"TRUMP\"]=1\n",
    "        else:\n",
    "            el[\"TRUMP\"]=0\n",
    "        if \"HILARY\" in supp:\n",
    "            el[\"HILARY\"]=1\n",
    "        else:\n",
    "            el[\"HILARY\"]=0\n",
    "        del el[\"topics\"]\n",
    "\n",
    "        db.res.insert_one(el)\n",
    "    return (graph, graph_dict)\n",
    "\n",
    "def make_rt_graph_for_nfakesres(cursor, max_seconds=False):\n",
    "    graph = nx.DiGraph()\n",
    "    graph_dict = dict()\n",
    "    for smth in cursor:\n",
    "        orig_id = ''\n",
    "        graph_dict[str(smth['msg_id'])] = smth\n",
    "        try: \n",
    "            orig_id = str(smth['retweeted_msg_id'])\n",
    "            orig_id = orig_id.split('_')[-1]\n",
    "            if max_seconds and smth['retweet_reaction_time_sec'] > max_seconds: # 5 minutes\n",
    "                continue\n",
    "        except KeyError: \n",
    "            continue\n",
    "        graph.add_edges_from([(orig_id, str(smth['msg_id']))])\n",
    "        \n",
    "        el = smth\n",
    "        try:\n",
    "            supp=list(flatten(el[\"topics\"]))\n",
    "        except KeyError:\n",
    "            continue\n",
    "        if \"TRUMP\" in supp:\n",
    "            el[\"TRUMP\"]=1\n",
    "        else:\n",
    "            el[\"TRUMP\"]=0\n",
    "        if \"HILARY\" in supp:\n",
    "            el[\"HILARY\"]=1\n",
    "        else:\n",
    "            el[\"HILARY\"]=0\n",
    "        del el[\"topics\"]\n",
    "\n",
    "        db.fres.insert_one(el)\n",
    "    return (graph, graph_dict)\n",
    "\n",
    "def make_rt_graph_from_collection(collection, max_seconds=False):\n",
    "    graph = nx.DiGraph()\n",
    "    graph_dict = dict()\n",
    "    cursor = ''\n",
    "    if max_seconds:\n",
    "        cursor = collection.find( { 'retweet_reaction_time_sec': {'$lt': int(max_seconds)}})\n",
    "    else:\n",
    "        cursor = collection.find()\n",
    "    for smth in cursor:\n",
    "        orig_id = ''\n",
    "        graph_dict[str(smth['msg_id'])] = smth\n",
    "        try: \n",
    "            orig_id = str(smth['retweeted_msg_id'])\n",
    "            orig_id = orig_id.split('_')[-1]\n",
    "        except KeyError: \n",
    "            continue\n",
    "        graph.add_edges_from([(orig_id, str(smth['msg_id']))])\n",
    "\n",
    "    return (graph, graph_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_graph = nx.DiGraph()\n",
    "fake_graph_dict = dict()\n",
    "(fake_graph, fake_graph_dict) = make_rt_graph_for_fakesres(db.fakes.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root(gra):\n",
    "     return nx.topological_sort(gra)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_comp(graph):\n",
    "    max_comp = 0\n",
    "    max_size = 0\n",
    "    for c in nx.weakly_connected_component_subgraphs(graph):\n",
    "        if (c.number_of_nodes() > max_size):\n",
    "            max_size = c.number_of_nodes()-1\n",
    "            max_comp = c\n",
    "    return (max_comp, max_size)\n",
    "\n",
    "def find_all_components(graph):\n",
    "    return list(nx.weakly_connected_component_subgraphs(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_cn = 0\n",
    "for gr in nx.weakly_connected_component_subgraphs(fake_graph):\n",
    "    root = find_root(gr)\n",
    "    s_cn += 1\n",
    "    for node in gr:\n",
    "        theEdges = list()\n",
    "        if node == root:\n",
    "            continue\n",
    "        for e in max_comp.edges():\n",
    "            if node in e:\n",
    "                theEdges.append(e)\n",
    "\n",
    "        if len(theEdges) > 1:\n",
    "            print(theEdges)\n",
    "            print(root)\n",
    "            \n",
    "\n",
    "print(\"Analyzed \", s_cn, \" subgraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 18))\n",
    "matplotlib.pyplot.subplot(121)\n",
    "nx.draw(max_comp, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_reshare_order(subgr, gr_dic):\n",
    "    time_msg_dict = dict()\n",
    "    for element in subgr.nodes():\n",
    "        try:\n",
    "            t = gr_dic[element]['retweet_reaction_time_sec']\n",
    "        except KeyError:\n",
    "            t = 0 # father\n",
    "        try:\n",
    "            time_msg_dict[t].append(element)\n",
    "        except KeyError:\n",
    "            time_msg_dict[t] = [element]\n",
    "    \n",
    "    ret = list()gr_dic[element]\n",
    "    for k in sorted(time_msg_dict.keys()):\n",
    "        ret.append((k,time_msg_dict[k]))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_reshare_order(max_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(fake_graph_dict.keys())[0]\n",
    "'802523214995931136' in fake_graph_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Temporal Features #\n",
    "\n",
    "def time_i(r_order, i): \n",
    "    try:\n",
    "        return r_order[i][0]\n",
    "    except IndexError:\n",
    "        return 0\n",
    "\n",
    "def time_first_half(r_order, k): \n",
    "    r_order = list(filter(lambda x: x[0] > 0, r_order)) # Do not count the 0-secs reshare of root\n",
    "    max_index = int(k/2)-1\n",
    "    lower_half_times = list()\n",
    "    for x in r_order[0:max_index]:\n",
    "        lower_half_times.append(x[0])\n",
    "    deltalow = list()\n",
    "    for i in range(len(lower_half_times)-1):\n",
    "        deltalow.append(lower_half_times[i+1]-lower_half_times[i])\n",
    "    if len(deltalow) > 0:\n",
    "        return sum(deltalow)/int(len(deltalow))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def time_second_half(r_order, k):\n",
    "    r_order = list(filter(lambda x: x[0] > 0, r_order)) # Do not count the 0-secs reshare of root\n",
    "    min_index = int(k/2)-1\n",
    "    top_half_times = list()\n",
    "    for x in r_order[min_index:int(k)-1]:\n",
    "        if x[0] > 0: # Do not count the 0-secs reshare of root\n",
    "            top_half_times.append(x[0])\n",
    "    deltatop = list()\n",
    "    for i in range(len(top_half_times)-1):\n",
    "        deltatop.append(top_half_times[i+1]-top_half_times[i])\n",
    "        \n",
    "    if len(deltatop) > 0:\n",
    "        return sum(deltatop)/int(len(deltatop))\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def time_change_reshares(r_order, k):\n",
    "    or_order = list(r_order)#list(filter(lambda x: x[0] > 0, r_order))\n",
    "    def f(x):\n",
    "        res = 0\n",
    "        t1 = 0\n",
    "        tp2 = 0\n",
    "        for i in range(or_order != [] and len(or_order)-1):\n",
    "            t1 = time_i(or_order, i)\n",
    "            tp1 = time_i(or_order, i+1) or t1\n",
    "            res += ((tp1-t1)-(1+i)*x)**2\n",
    "        return res\n",
    "    x0 = 0.01\n",
    "    return minimize(fun=f, x0=x0).x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_order = find_reshare_order(max_comp)\n",
    "max_k=len(res_order) -1\n",
    "print(time_first_half(res_order, max_k))\n",
    "print(time_second_half(res_order, max_k))\n",
    "print(time_change_reshares(r_order, max_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple conversion\n",
    "def time_str_to_datetime(time_str):\n",
    "    return datetime.datetime.strptime(time_str, '%a %b %d %H:%M:%S %z %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_order = find_reshare_order(max_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check date format\n",
    "db.fakes.find_one()['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = pytz.utc.localize(datetime.datetime.utcfromtimestamp(0))\n",
    "times_dt = list(map(lambda x: time_str_to_datetime(x['datetime']), db.fakes.find({}, {'datetime':1, '_id':0})))\n",
    "times = list(map(lambda x: (time_str_to_datetime(x['datetime'])-epoch).total_seconds(), db.fakes.find({}, {'datetime':1, '_id':0})))\n",
    "t_min = min(times)\n",
    "times = list(map(lambda x: x-t_min, times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# For non fakes #\n",
    "\n",
    "asdasdasdrg()\n",
    "date_dict_nonfakes = dict()\n",
    "for date in db.nonfakes.find({}, {'datetime':1, '_id':0}):\n",
    "    try: \n",
    "        date_dict_nonfakes[time_str_to_datetime(date['datetime'])] += 1\n",
    "    except KeyError:\n",
    "        date_dict_nonfakes[time_str_to_datetime(date['datetime'])] = 1\n",
    "date_dts_nf = list(date_dict_nonfakes.keys())\n",
    "values_nf = list()\n",
    "for d in date_dts_nf:\n",
    "    values.append(date_dict_nonfakes[d])\n",
    "\n",
    "dates_nf = matplotlib.dates.date2num(date_dts_nf)\n",
    "matplotlib.pyplot.plot_date(dates_nf, values_nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.nonfakes.create_index([('datetime', pym.ASCENDING)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# For non fakes #\n",
    "\n",
    "date_dict_nonfakes = dict()\n",
    "c=0\n",
    "for date in db.nonfakes.find({}, {'datetime':1, '_id':0}):\n",
    "    c+=1\n",
    "    try: \n",
    "        date_dict_nonfakes[time_str_to_datetime(date['datetime'])] += 1\n",
    "    except KeyError:\n",
    "        date_dict_nonfakes[time_str_to_datetime(date['datetime'])] = 1\n",
    "print(c)\n",
    "print('done for')\n",
    "date_dts_nf = list(date_dict_nonfakes.keys())\n",
    "values_nf = list()\n",
    "print('start appending')\n",
    "for d in date_dts_nf:\n",
    "    values_nf.append(date_dict_nonfakes[d])\n",
    "\n",
    "dates_nf = matplotlib.dates.date2num(date_dts_nf)\n",
    "matplotlib.pyplot.plot_date(dates_nf, values_nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_dict = dict()\n",
    "for date in db.fakes.find({}, {'datetime':1, '_id':0}):\n",
    "    try: \n",
    "        date_dict[time_str_to_datetime(date['datetime'])] += 1\n",
    "    except KeyError:\n",
    "        date_dict[time_str_to_datetime(date['datetime'])] = 1\n",
    "date_dts = list(date_dict.keys())\n",
    "values = list()\n",
    "for d in date_dts:\n",
    "    values.append(date_dict[d])\n",
    "dates = matplotlib.dates.date2num(date_dts)\n",
    "matplotlib.pyplot.plot_date(dates, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_dict = dict()\n",
    "for date in db.bias_tweets_unique.find({}, {'datetime':1, '_id':0}):\n",
    "    try: \n",
    "        date_dict[time_str_to_datetime(date['datetime'])] += 1\n",
    "    except KeyError:\n",
    "        date_dict[time_str_to_datetime(date['datetime'])] = 1\n",
    "date_dts = list(date_dict.keys())\n",
    "values = list()\n",
    "for d in date_dts:\n",
    "    values.append(date_dict[d])\n",
    "dates = matplotlib.dates.date2num(date_dts)\n",
    "matplotlib.pyplot.plot_date(dates, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def component_percent_rt_after_k(comp, k):\n",
    "    order = find_reshare_order(comp)\n",
    "    count = 0\n",
    "    for x in order:\n",
    "        if x[0] > k:\n",
    "            break\n",
    "        else:\n",
    "            count+=1\n",
    "    return count/len(order)\n",
    "\n",
    "def total_percent_rt_after_k(graph, k):\n",
    "    tot = 0\n",
    "    comps = find_all_components(graph)\n",
    "    for subg in comps:\n",
    "        tot += component_percent_rt_after_k(subg, k)\n",
    "    return tot/len(comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_percent_rt_after_k(make_rt_graph(db.fakes.find(), 60)[0], 30)*100, '%', sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_graph = make_rt_graph_from_collection(db.nonfakes, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdasddswef()\n",
    "current_elements = list()\n",
    "res_graph = nx.DiGraph()\n",
    "max_len = 100000\n",
    "for c,el in enumerate(db.nonfakes.find()):\n",
    "    current_elements.append(el)\n",
    "    if(c and c%max_len == 0):    \n",
    "        res_graph = nx.compose(res_graph, make_rt_graph(current_elements)[0])\n",
    "        current_elements = list()\n",
    "res_graph = nx.compose(res_graph, make_rt_graph(current_elements)[0])\n",
    "print(len(res_graph.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_percent_rt_after_k(nf_graph[0], 10)*100, '%', sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(res_graph.edges()))\n",
    "print(len(res_graph.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_reshare_order(find_max_comp(nf_graph[0])[0])\n",
    "print(0%90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdasdasd()\n",
    "\n",
    "percentages = list()\n",
    "for i in range(20):\n",
    "    nf_graph = make_rt_graph(db.nonfakes.aggregate([ { '$sample': { 'size': 100000 } } ]))\n",
    "    percentages.append(total_percent_rt_after_k(nf_graph[0], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(percentages)/len(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_percent_rt_after_k(res_graph, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_comp_4m=find_max_comp(res_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_ord_4m = find_reshare_order(max_comp_4m[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_comp_4m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in max_comp_4m[0].nodes():\n",
    "    print(node)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutti_figli = list()\n",
    "max_iter = 40000\n",
    "print(datetime.datetime.now())\n",
    "for c,ele in enumerate(db.nonfakes.find({'retweet_reaction_time_sec': {'$gte':0}}).limit(max_iter)):\n",
    "    tutti_figli.append(ele)\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tutti_figli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "father_son = dict()\n",
    "for f in tutti_figli:\n",
    "    try:\n",
    "        father_son[f['retweeted_msg_id']].append(f)\n",
    "    except KeyError:\n",
    "        father_son[f['retweeted_msg_id']] = [f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_family = max(map(len,father_son.values()))\n",
    "smallest_family = min(map(len,father_son.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Biggest family: ', biggest_family)\n",
    "print('Smallest family: ',smallest_family)\n",
    "print('Average family size: ', sum(list(map(len,father_son.values())))/len(father_son.values()))\n",
    "print('Families: ', len(father_son.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "copia_figli =  copy.deepcopy(tutti_figli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db.res.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "fake_1000_makert = make_rt_graph(tutti_figli)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfake_graph = nx.DiGraph()\n",
    "nfake_graph_dict = dict()\n",
    "(nfake_graph, nfake_graph_dict) = fake_1000_makert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_comp=find_max_comp(fake_graph)[0]\n",
    "c = 0\n",
    "chg = list()\n",
    "for comp in (nx.weakly_connected_component_subgraphs(nfake_graph)):\n",
    "    res_order = find_reshare_order(comp, nfake_graph_dict)\n",
    "    max_k= min(5, len(res_order[0][1]) - 1)\n",
    "    tc = time_change_reshares(res_order, max_k)\n",
    "    first = time_first_half(res_order, max_k)\n",
    "    second = time_second_half(res_order, max_k)\n",
    "    i_time = time_i(res_order, max_k)\n",
    "    for node in comp.nodes():\n",
    "        db.res.update_one({'msg_id':Int64(str(node))}, {'$set':\n",
    "                                                {   'tmp_feat_k':max_k,\n",
    "                                                    'tmp_feat_tk':i_time, \n",
    "                                                    'tmp_feat_firstk':first,\n",
    "                                                    'tmp_feat_secondk':second,\n",
    "                                                    'tmp_feat_time_chg':tc\n",
    "                                                }\n",
    "                                               })\n",
    "        \n",
    "    chg.append(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(chg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avg = list()\n",
    "for comp in (nx.weakly_connected_component_subgraphs(nfake_graph)):\n",
    "    res_order = find_reshare_order(comp, nfake_graph_dict)\n",
    "    media = 0\n",
    "    mm  = map(lambda x: [x[0]]*len(x[1]), res_order)\n",
    "    ll = list(mm)\n",
    "    avg = sum((flatten(ll)))/len(ll)\n",
    "    all_avg.append((len(ll), avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_t = 0\n",
    "tot_cn = 0\n",
    "min_t = 100000000000000000000000000000000000\n",
    "max_t = 0\n",
    "for w,a in all_avg:\n",
    "    tot_t += w*a\n",
    "    tot_cn += w\n",
    "    if a < min_t and a > 0:\n",
    "        min_t = a\n",
    "    if a > max_t:\n",
    "        max_t = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_all = list(flatten(map(lambda x: x[0]*[x[1]], all_avg)))\n",
    "#nfake_graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = nx.Graph()\n",
    "dummy.add_edges_from([('0','1'), ('0','2'), ('0','3'), ('0', '4'), ('0', '5')])\n",
    "\n",
    "#dummy = nx.complete_graph(10)\n",
    "\n",
    "nx.wiener_index(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.mode(flattened_all))\n",
    "stats.describe(flattened_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfwiener_total = dict()\n",
    "\n",
    "for comp in (nx.weakly_connected_component_subgraphs(nfake_graph)):\n",
    "    undir = nx.Graph()\n",
    "    undir.add_edges_from(comp.edges())\n",
    "    f_wien = nx.wiener_index(undir)\n",
    "    for node in comp.nodes():\n",
    "        nfwiener_total[node] = f_wien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prova(el,string):\n",
    "    try:\n",
    "        return el[string]\n",
    "    except KeyError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index =  db.res.create_index([('msg_id', pym.ASCENDING)], unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(wiener_total.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiener_median = statistics.median(nfwiener_total.values())\n",
    "c = 0\n",
    "for node in nfwiener_total.keys():\n",
    "    is_gt = int(nfwiener_total[node]>wiener_median)\n",
    "    db.res.update_one({'msg_id':Int64(str(node))}, {'$set':\n",
    "                                                {   'wiener_gt_median':is_gt,\n",
    "                                                }\n",
    "                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_x_list_component(el):\n",
    "    return [\n",
    "                prova(el,'is_manual_reply'), \n",
    "                prova(el,'is_manual_retweet'),\n",
    "                prova(el,'is_quote_retweet'), \n",
    "                prova(el,'is_reply_button'), \n",
    "                prova(el,'is_retweet_button'),\n",
    "                prova(el,'TRUMP'), \n",
    "                prova(el,'HILARY'), \n",
    "                prova(el,'is_retweeted_user_verified'), \n",
    "                prova(el,'retweet_reaction_time_sec'),  \n",
    "                prova(el,'retweeted_user_followers_count'),\n",
    "                prova(el,'tmp_feat_k'),\n",
    "                prova(el,'tmp_feat_tk'),\n",
    "                prova(el,'tmp_feat_firstk'),\n",
    "                prova(el,'tmp_feat_secondk'),\n",
    "                prova(el,'tmp_feat_time_chg'),\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=list()\n",
    "Y=list()\n",
    "\n",
    "for el in db.res.find():\n",
    "    Y.append(prova(el,'wiener_gt_median'))\n",
    "    X.append(compute_x_list_component(el))\n",
    "\n",
    "logreg = linear_model.LogisticRegression(n_jobs=4)\n",
    "logreg.fit(X[10000:], Y[10000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = X[:10000]\n",
    "y_test = Y[:10000]\n",
    "y_pred = logreg.predict(np.array(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Acc', accuracy_score(y_pred=y_pred, y_true=y_test))\n",
    "print('F1', f1_score(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftutti_figli = list()\n",
    "print(datetime.datetime.now())\n",
    "for c,ele in enumerate(db.fakes.find()):\n",
    "    ftutti_figli.append(ele)\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffather_son = dict()\n",
    "for f in ftutti_figli:\n",
    "    try:\n",
    "        ffather_son[f['retweeted_msg_id']].append(f)\n",
    "    except KeyError:\n",
    "        try:\n",
    "            ffather_son[f['retweeted_msg_id']] = [f]\n",
    "        except KeyError:\n",
    "            ffather_son[f['msg_id']] = [f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbiggest_family = max(map(len,ffather_son.values()))\n",
    "fsmallest_family = min(map(len,ffather_son.values()))\n",
    "print('Biggest family: ', fbiggest_family)\n",
    "print('Smallest family: ',fsmallest_family)\n",
    "print('Average family size: ', sum(list(map(len,ffather_son.values())))/len(ffather_son.values()))\n",
    "print('Families: ', len(ffather_son.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db.fres.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "nfake_1000_makert = make_rt_graph_for_fakesres(ftutti_figli)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_graph = nx.DiGraph()\n",
    "fake_graph_dict = dict()\n",
    "(fake_graph, fake_graph_dict) = nfake_1000_makert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_comp=find_max_comp(fake_graph)[0]\n",
    "c = 0\n",
    "chg = list()\n",
    "for comp in (nx.weakly_connected_component_subgraphs(fake_graph)):\n",
    "    res_order = find_reshare_order(comp, fake_graph_dict)\n",
    "    max_k= min(5, len(res_order[0][1]) - 1)\n",
    "    tc = time_change_reshares(res_order, max_k)\n",
    "    first = time_first_half(res_order, max_k)\n",
    "    second = time_second_half(res_order, max_k)\n",
    "    i_time = time_i(res_order, max_k)\n",
    "    for node in comp.nodes():\n",
    "        db.fres.update_one({'msg_id':Int64(str(node))}, {'$set':\n",
    "                                                {   'tmp_feat_k':max_k,\n",
    "                                                    'tmp_feat_tk':i_time, \n",
    "                                                    'tmp_feat_firstk':first,\n",
    "                                                    'tmp_feat_secondk':second,\n",
    "                                                    'tmp_feat_time_chg':tc\n",
    "                                                }\n",
    "                                               })\n",
    "        \n",
    "    chg.append(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(chg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwiener_total = dict()\n",
    "\n",
    "for comp in (nx.weakly_connected_component_subgraphs(fake_graph)):\n",
    "    undir = nx.Graph()\n",
    "    undir.add_edges_from(comp.edges())\n",
    "    f_wien = nx.wiener_index(undir)\n",
    "    for node in comp.nodes():\n",
    "        fwiener_total[node] = f_wien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prova(el,string):\n",
    "    try:\n",
    "        return el[string]\n",
    "    except KeyError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index =  db.fres.create_index([('msg_id', pym.ASCENDING)], unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiener_median = statistics.median(fwiener_total.values())\n",
    "c = 0\n",
    "for node in fwiener_total.keys():\n",
    "    is_gt = int(fwiener_total[node]>wiener_median)\n",
    "    db.fres.update_one({'msg_id':Int64(str(node))}, {'$set':\n",
    "                                                {   'wiener_gt_median':is_gt,\n",
    "                                                }\n",
    "                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fX=list()\n",
    "fY=list()\n",
    "\n",
    "for el in db.fres.find():\n",
    "    fY.append(prova(el,'wiener_gt_median'))\n",
    "    fX.append(compute_x_list_component(el))\n",
    "\n",
    "fake_logreg = linear_model.LogisticRegression(n_jobs=4)\n",
    "fake_logreg.fit(fX[4000:], fY[4000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = fX[:4000]\n",
    "y_test = fY[:4000]\n",
    "y_pred = fake_logreg.predict(np.array(x_test))\n",
    "\n",
    "print('Acc', accuracy_score(y_pred=y_pred, y_true=y_test))\n",
    "print('F1', f1_score(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fake_logreg.get_params())\n",
    "print(logreg.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model_selection.cross_val_score(fake_logreg, fX, fY, cv=10, scoring='accuracy')\n",
    "print(evaluation.mean(), evaluation.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model_selection.cross_val_score(logreg, X, Y, cv=10, scoring='accuracy')\n",
    "print(evaluation.mean(), evaluation.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# MISCHIANDO\n",
    "\n",
    "y_pred = fake_logreg.predict(np.array(x_test))\n",
    "y_test = Y[:10000]\n",
    "print('Acc', accuracy_score(y_pred=y_pred, y_true=y_test))\n",
    "print('F1', f1_score(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# MISCHIANDO\n",
    "\n",
    "y_pred = logreg.predict(np.array(fX[:4000]))\n",
    "y_test = fY[:4000]\n",
    "\n",
    "\n",
    "print('Accuracy', accuracy_score(y_pred=y_pred, y_true=y_test))\n",
    "print('F1', f1_score(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(Y)/len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('fake wiener', \n",
    "      statistics.mean(fwiener_total.values()), \n",
    "      statistics.mode(fwiener_total.values()), \n",
    "      statistics.stdev(fwiener_total.values()),\n",
    "      statistics.median(fwiener_total.values()))\n",
    "print('nonfake wiener', \n",
    "      statistics.mean(wiener_total.values()), \n",
    "      statistics.mode(wiener_total.values()), \n",
    "      statistics.stdev(wiener_total.values()),\n",
    "      statistics.median(wiener_total.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = 1\n",
    "tpr = 1\n",
    "fpr,tpr,_ =  roc_curve(y_test, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_logreg.fit(fX, fY).decision_function(fX[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for the_id in db.fakes.find({}, {'msg_id':1, '_id':0}):\n",
    "    if db.follows.find({'[on':int(the_id['msg_id'])}).count() > 0:\n",
    "        print(the_id)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in db.follows.find():\n",
    "    print (el['[on'], type(el['[on']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average edges\n",
    "nfoutd = list()\n",
    "nfnodes = list()\n",
    "for comp in (nx.weakly_connected_component_subgraphs(nfake_graph)):\n",
    "    nfoutd.append(len(comp.edges()))\n",
    "    nfnodes.append(len(comp.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics.mean(nfnodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average edges\n",
    "foutd = list()\n",
    "fnodes = list()\n",
    "for comp in (nx.weakly_connected_component_subgraphs(fake_graph)):\n",
    "    foutd.append(len(comp.edges()))\n",
    "    fnodes.append(len(comp.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.n_components = 6\n",
    "X_red = pca.fit_transform(X)\n",
    "X_red.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model_selection.cross_val_score(logreg, X, Y, cv=10, scoring='accuracy')\n",
    "print(evaluation.mean(), evaluation.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(n_jobs=4)\n",
    "dbscan_result = dbscan.fit_predict(X+fX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(dbscan_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in set(dbscan_result):\n",
    "    print(el, ': ', np.count_nonzero(dbscan_result == int(el)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dbscan_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = KMeans(n_clusters=20, n_jobs=4)\n",
    "kmeans_result = kmeans_model.fit_predict(X+fX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster,element in zip(kmeans_result, X+fX):\n",
    "    if cluster == 2:\n",
    "        print(cluster, ' : ', element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in set(kmeans_result):\n",
    "    print(el, ': ', np.count_nonzero(kmeans_result == int(el)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_X = list()\n",
    "for el in X+fX:\n",
    "    less_X.append(el[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan2 = DBSCAN(n_jobs=4)\n",
    "dbscan_result2 = dbscan2.fit_predict(less_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in set(dbscan_result2):\n",
    "    print(el, ': ', np.count_nonzero(dbscan_result2 == int(el)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model2 = KMeans(n_clusters=20, n_jobs=4)\n",
    "kmeans_result2 = kmeans_model.fit_predict(less_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in set(kmeans_result2):\n",
    "    print(el, ': ', np.count_nonzero(kmeans_result2 == int(el)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}