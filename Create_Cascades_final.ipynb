{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pymongo as pym\n",
    "import seaborn as sns\n",
    "from bson.objectid import ObjectId\n",
    "from bson import Int64\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from pylab import *\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import independent_cascade as ind_casc\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "from matplotlib.cbook import flatten\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "from matplotlib.cbook import flatten\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import statistics\n",
    "\n",
    "from sklearn import model_selection\n",
    "import sklearn\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import rake\n",
    "import rake2\n",
    "import operator\n",
    "import string\n",
    "\n",
    "QUERY_LIMIT = 60000\n",
    "global generated_graph_components\n",
    "generated_graph_components = None\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.faken\n",
    "coll = db.bias_tweets_unique\n",
    "cursor = db.bias_tweets_unique.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascade_representatives = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeGraphDictFeatures(cursor, max_seconds=False):\n",
    "    graph = nx.DiGraph()\n",
    "    graph_dict = dict()\n",
    "    for smth in cursor:\n",
    "        orig_id = ''\n",
    "        \n",
    "        try: \n",
    "            orig_id = str(smth['retweeted_msg_id'])\n",
    "            orig_id = orig_id.split('_')[-1]\n",
    "            if max_seconds and smth['retweet_reaction_time_sec'] > max_seconds: # 5 minutes\n",
    "                continue\n",
    "            graph.add_edges_from([(orig_id, str(smth['msg_id']))])\n",
    "        except KeyError: \n",
    "            graph.add_node(str(smth['msg_id']))\n",
    "            \n",
    "        \n",
    "        \n",
    "        el = smth\n",
    "        try:\n",
    "            supp=list(flatten(el[\"topics\"]))\n",
    "        except KeyError:\n",
    "            supp = []\n",
    "        if \"TRUMP\" in supp:\n",
    "            el[\"TRUMP\"]=1\n",
    "        else:\n",
    "            el[\"TRUMP\"]=0\n",
    "        if \"HILARY\" in supp:\n",
    "            el[\"HILARY\"]=1\n",
    "        else:\n",
    "            el[\"HILARY\"]=0\n",
    "        del el[\"topics\"]\n",
    "        graph_dict[str(smth['msg_id'])] = smth\n",
    "        \n",
    "    return (graph, graph_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_reshare_order(subgr, gr_dic):\n",
    "    time_msg_dict = dict()\n",
    "    for element in subgr.nodes():\n",
    "        try:\n",
    "            t = gr_dic[element]['retweet_reaction_time_sec']\n",
    "        except KeyError:\n",
    "            t = 0 # father\n",
    "        try:\n",
    "            time_msg_dict[t].append(element)\n",
    "        except KeyError:\n",
    "            time_msg_dict[t] = [element]\n",
    "    \n",
    "    ret = list()\n",
    "    gr_dic[element]\n",
    "    for k in sorted(time_msg_dict.keys()):\n",
    "        ret.append((k,time_msg_dict[k]))\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Temporal Features #\n",
    "\n",
    "def time_i(r_order, i): \n",
    "    try:\n",
    "        return r_order[i][0]\n",
    "    except IndexError:\n",
    "        return 0\n",
    "\n",
    "def time_first_half(r_order, k): \n",
    "    r_order = list(filter(lambda x: x[0] > 0, r_order)) # Do not count the 0-secs reshare of root\n",
    "    max_index = int(k/2)-1\n",
    "    lower_half_times = list()\n",
    "    for x in r_order[0:max_index]:\n",
    "        lower_half_times.append(x[0])\n",
    "    deltalow = list()\n",
    "    for i in range(len(lower_half_times)-1):\n",
    "        deltalow.append(lower_half_times[i+1]-lower_half_times[i])\n",
    "    if len(deltalow) > 0:\n",
    "        return sum(deltalow)/int(len(deltalow))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def time_second_half(r_order, k):\n",
    "    r_order = list(filter(lambda x: x[0] > 0, r_order)) # Do not count the 0-secs reshare of root\n",
    "    min_index = int(k/2)-1\n",
    "    top_half_times = list()\n",
    "    for x in r_order[min_index:int(k)-1]:\n",
    "        if x[0] > 0: # Do not count the 0-secs reshare of root\n",
    "            top_half_times.append(x[0])\n",
    "    deltatop = list()\n",
    "    for i in range(len(top_half_times)-1):\n",
    "        deltatop.append(top_half_times[i+1]-top_half_times[i])\n",
    "        \n",
    "    if len(deltatop) > 0:\n",
    "        return sum(deltatop)/int(len(deltatop))\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def time_change_reshares(r_order, k):\n",
    "    or_order = list(r_order)#list(filter(lambda x: x[0] > 0, r_order))\n",
    "    def f(x):\n",
    "        res = 0\n",
    "        t1 = 0\n",
    "        tp2 = 0\n",
    "        for i in range(or_order != [] and len(or_order)-1):\n",
    "            t1 = time_i(or_order, i)\n",
    "            tp1 = time_i(or_order, i+1) or t1\n",
    "            res += ((tp1-t1)-(1+i)*x)**2\n",
    "        return res\n",
    "    x0 = 0.01\n",
    "    return minimize(fun=f, x0=x0).x[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findComponents(graph, graph_dict):\n",
    "    result = dict()\n",
    "    father = ''\n",
    "    for node in graph.nodes():\n",
    "        try:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAllChildren(cursor):\n",
    "    ftutti_figli = list()\n",
    "    print(datetime.datetime.now())\n",
    "    print(\"Make children list\")\n",
    "    \n",
    "    for ele in cursor:\n",
    "        ftutti_figli.append(ele)\n",
    "\n",
    "    print(datetime.datetime.now())\n",
    "    return ftutti_figli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFamilies(children_list):\n",
    "    print(datetime.datetime.now())\n",
    "    print(\"Start making all families\")\n",
    "\n",
    "    ffather_son = dict()\n",
    "    for f in children_list:\n",
    "        try:\n",
    "            ffather_son[f['retweeted_msg_id']].append(f)\n",
    "        except KeyError:\n",
    "            try:\n",
    "                ffather_son[f['retweeted_msg_id']] = [f]\n",
    "            except KeyError:\n",
    "                ffather_son[f['msg_id']] = [f]\n",
    "\n",
    "    print(datetime.datetime.now())\n",
    "    return ffather_son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wienerIndexDictionary(graph):\n",
    "    fwiener_total = dict()\n",
    "    print(\"Starting Wiener index computations\")\n",
    "    print(datetime.datetime.now())\n",
    "        \n",
    "    for comp in nx.weakly_connected_component_subgraphs(graph):\n",
    "        undir = nx.Graph()\n",
    "        undir.add_edges_from(comp.edges())\n",
    "        undir.add_nodes_from(comp.nodes())\n",
    "        f_wien = nx.wiener_index(undir)\n",
    "        for node in comp.nodes():\n",
    "            fwiener_total[node] = f_wien\n",
    "    print(datetime.datetime.now())\n",
    "    return fwiener_total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTimeWienerFeaturesPartial(graph, graph_dict, wiener_dict):\n",
    "    wiener_median = statistics.median(wiener_dict.values())\n",
    "\n",
    "    for comp in nx.weakly_connected_component_subgraphs(graph):\n",
    "        res_order = find_reshare_order(comp, graph_dict)\n",
    "        max_k= min(5, len(res_order[0][1]) - 1)\n",
    "        tc = time_change_reshares(res_order, max_k)\n",
    "        first = time_first_half(res_order, max_k)\n",
    "        second = time_second_half(res_order, max_k)\n",
    "        i_time = time_i(res_order, max_k)\n",
    "        for node in comp.nodes():\n",
    "            try:\n",
    "                el = graph_dict[node] \n",
    "            except KeyError:\n",
    "                continue\n",
    "            el['tmp_feat_k'] = max_k\n",
    "            el['tmp_feat_tk'] = i_time\n",
    "            el['tmp_feat_firstk'] = first\n",
    "            el['tmp_feat_secondk'] = second\n",
    "            el['tmp_feat_time_chg'] = tc\n",
    "            el['wiener_gt_median'] = int(wiener_dict[node]>wiener_median)\n",
    "            el['wiener_index'] = wiener_dict[node]\n",
    "            \n",
    "            cascade_representatives.append(el)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_everything(list_cursors, list_isfake, cascade_representatives):\n",
    "    \n",
    "    for cur, flag in zip(list_cursors, list_isfake):\n",
    "        print(\"Starting all!!!\")\n",
    "        print(datetime.datetime.now())\n",
    "        generated_graph_components = None\n",
    "        children = findAllChildren(cur)\n",
    "        (generated_graph, generated_dict) = makeGraphDictFeatures(children)\n",
    "        if generated_graph_components is None:\n",
    "            generated_graph_components = nx.weakly_connected_component_subgraphs(generated_graph)\n",
    "        wienerDict = wienerIndexDictionary(graph=generated_graph)\n",
    "        addTimeWienerFeaturesPartial(generated_graph, generated_dict, wienerDict)\n",
    "        print(\"All done!!\")\n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "        wiener_median = statistics.median(wienerDict.values())\n",
    "        print(wiener_median)\n",
    "\n",
    "        print('Wiener stats', \n",
    "              statistics.mean(wienerDict.values()), \n",
    "              statistics.mode(wienerDict.values()), \n",
    "              statistics.stdev(wienerDict.values()),\n",
    "              statistics.median(wienerDict.values()),\n",
    "              max(wienerDict.values()))\n",
    "    def mapLocF(x):\n",
    "        try:\n",
    "            x['fake_news']\n",
    "        except KeyError:\n",
    "            x['fake_news'] = flag\n",
    "        return x\n",
    "    cascade_representatives = list(map(mapLocF, cascade_representatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascade_representatives = []\n",
    "cursors = [db.fakes.find(), db.nonfakes.find().limit(QUERY_LIMIT)]\n",
    "fake_flags = [1,0]\n",
    "do_everything(cursors, fake_flags,cascade_representatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ONCE!!\n",
    "careasdasd() # dummy function to force you to read this! Run once only as it may overwrite an useful backup!\n",
    "__cascade_complete_backup = copy.deepcopy(cascade_representatives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascade_representatives = copy.deepcopy(__cascade_complete_backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(cascade_representatives))\n",
    "print(statistics.mean(list(map(lambda x: x['wiener_index'], cascade_representatives))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedStopWords = stopwords.words(\"english\")\n",
    "lemmatiser = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeText=''\n",
    "s=''\n",
    "for el in cascade_representatives:\n",
    "    text = el['text']\n",
    "    text = ' '.join([word for word in text.split() if word not in cachedStopWords and not word.startswith('#') and not word.startswith('@') and not word.startswith('http')])\n",
    "    for word in text.split():\n",
    "        s+=lemmatiser.lemmatize(word, pos=\"v\")+' '\n",
    "translator = str.maketrans('', '', string.punctuation)    \n",
    "rake_object = rake.Rake('SmartStoplist.txt',3, 1, 15)\n",
    "keywords = rake_object.run(s.translate(translator))\n",
    "print (\"Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in fn_words:\n",
    "    if w in keywords_clean:\n",
    "        print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_clean = list(map(lambda x: x[0],keywords))\n",
    "len(keywords_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cascade_representatives[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in cascade_representatives:\n",
    "    for key in keywords_clean:\n",
    "        if key in el['text']:\n",
    "            el[str(key)]=1\n",
    "        else:\n",
    "            el[str(key)] = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "__cascade_backup =  copy.deepcopy(cascade_representatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascade_representatives = copy.deepcopy(__cascade_backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list()\n",
    "z = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prohib_key = ['text', 'author', 'datetime', 'msg_id', 'is_manual_reply', 'is_manual_retweet', \n",
    "              'is_quote_retweet','is_reply_button', 'is_retweet_button', 'wiener_gt_median', 'wiener_index',\n",
    "              'tmp_feat_k', 'tmp_feat_tk', 'tmp_feat_firstk', 'tmp_feat_secondk', 'tmp_feat_time_chg',\n",
    "              'retweet_reaction_time_sec', 'retweeted_user', 'retweeted_user_followers_count', 'replied_user', \n",
    "              'retweeted_msg_id'\n",
    "             ]\n",
    "len(prohib_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shish(x):\n",
    "    x['_id'] = str(x['_id'])\n",
    "    return x\n",
    "cascade_representatives = list(map(shish, cascade_representatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in cascade_representatives:\n",
    "    try:\n",
    "        el['is_retweeted_user_verified']\n",
    "    except KeyError:\n",
    "        el['is_retweeted_user_verified'] = False\n",
    "    for key in prohib_key:\n",
    "        try:\n",
    "            del el[key]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    y.append(el['fake_news'])\n",
    "    z.append(el['_id'])\n",
    "    del el['fake_news']\n",
    "    del el['_id']\n",
    "    \n",
    "def create_sorted_list(x):\n",
    "    ret = list()\n",
    "    for k in sorted(x.keys()):\n",
    "        ret.append(x[k])\n",
    "    return ret\n",
    "\n",
    "cascade_representatives = list(map(create_sorted_list, cascade_representatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = linear_model.LogisticRegression(n_jobs = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(cascade_representatives, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_proba = dict()\n",
    "for theId,proba in zip(z, logreg.predict_proba(cascade_representatives)):\n",
    "    id_proba[theId] = proba[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_ps = list()\n",
    "for tw in __cascade_complete_backup:\n",
    "    new = copy.deepcopy(tw)\n",
    "    new['propensity_score'] = id_proba[str(new['_id'])]\n",
    "    tweet_ps.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tw in tweet_ps:\n",
    "#    db.propensity_40.insert_one(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.distplot(list(map(lambda x: x['propensity_score'], tweet_ps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(list(map(lambda x: int(x['propensity_score']<0.3), tweet_ps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = dict()\n",
    "N_bins = 20\n",
    "for el in tweet_ps:\n",
    "    \n",
    "    for prop in np.linspace(0,1,N_bins):\n",
    "        if el['propensity_score'] < prop:\n",
    "            try:\n",
    "                bins[prop].append(el)\n",
    "            except KeyError:\n",
    "                bins[prop] = [el]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in sorted(bins.keys()):\n",
    "    print(l,len(bins[l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_w = dict()\n",
    "non_fake_w = dict()\n",
    "c=0\n",
    "for k in bins.keys():\n",
    "    fake_w[k] = []\n",
    "    non_fake_w[k] = []\n",
    "    for el in bins[k]:\n",
    "        if el['fake_news']:\n",
    "            fake_w[k].append(el['wiener_index'])\n",
    "        else:\n",
    "            non_fake_w[k].append(el['wiener_index'])\n",
    "            \n",
    "for k in sorted(bins.keys()):\n",
    "    print(k, \"Fake: \", statistics.mean(fake_w[k]), \"#fake: \", len(fake_w[k]))\n",
    "    print(k, \"NonF: \", statistics.mean(non_fake_w[k]), len(non_fake_w[k]), end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in bins.keys():\n",
    "    if not str(k).startswith('0.15'):\n",
    "        continue\n",
    "   \n",
    "    for el in bins[k]:\n",
    "        if el['fake_news']:\n",
    "            print(el)\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = list()\n",
    "nf = list()\n",
    "for k in bins.keys():\n",
    "    fa.append(statistics.mean(fake_w[k]))\n",
    "    nf.append(statistics.mean(non_fake_w[k]))\n",
    "print(\"Fake:\", statistics.mean(fa))\n",
    "print(\"NFake:\", statistics.mean(nf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fak = 0\n",
    "nfak = 0\n",
    "for k in bins.keys():\n",
    "    if statistics.mean(fake_w[k]) > statistics.mean(non_fake_w[k]):\n",
    "        fak+=1\n",
    "    else:\n",
    "        nfak +=1\n",
    "        \n",
    "print(fak, nfak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg2 = linear_model.LogisticRegression(n_jobs = 4)\n",
    "thr = int(2*len(cascade_representatives)/3)\n",
    "x_train = cascade_representatives[:thr]\n",
    "x_test = cascade_representatives[thr:]\n",
    "y_train = y[:thr]\n",
    "y_test = y[thr:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(logreg2, cascade_representatives, y, cv=10)\n",
    "statistics.mean(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics.mean(scores), statistics.stdev(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_tweets = dict()\n",
    "non_fake_tweets = dict()\n",
    "c=0\n",
    "for k in bins.keys():\n",
    "    fake_tweets[k] = []\n",
    "    non_fake_tweets[k] = []\n",
    "    for el in bins[k]:\n",
    "        print(el)\n",
    "        break\n",
    "        if el['fake_news']:\n",
    "            fake_tweets[k].append(el)\n",
    "        else:\n",
    "            non_fake_tweets[k].append(el)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_tweets = dict()\n",
    "non_fake_tweets = dict()\n",
    "c=0\n",
    "for k in bins.keys():\n",
    "    fake_tweets[k] = []\n",
    "    non_fake_tweets[k] = []\n",
    "    for el in bins[k]:\n",
    "        try:\n",
    "            if el['fake_news']:\n",
    "                fake_tweets[k].append(el)\n",
    "            else:\n",
    "                non_fake_tweets[k].append(el)\n",
    "        except KeyError:\n",
    "            non_fake_tweets[k].append(el)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
